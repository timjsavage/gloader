{
  "name": "elasticsearch",
  "description": "Installs and configures elasticsearch on Amazon EC2",
  "long_description": "Description\n-----------\n\nThis cookbook installs and configures the [_elasticsearch_](http://www.elasticsearch.org) search engine/database.\n\nIt requires a working _Java_ installation on the target node; add your preferred `java` cookbook to the node `run_list`.\n\nThe cookbook downloads the _elasticsearch_ tarball from GitHub (via the [`ark`](http://github.com/bryanwb/chef-ark) provider),\nunpacks and moves it to the directory you have specified in the node configuration (`/usr/local/elasticsearch` by default).\n\nIt installs a service which enables you to start, stop, restart and check status of the _elasticsearch_ process.\n\nIf your node has the `monit` recipe in its `run_list`, it will also create a configuration file for _Monit_,\nwhich will check whether _elasticsearch_ is running, reachable by HTTP and the cluster is in the “green” state.\n\nIf you include the `elasticsearch::plugin_aws` recipe, the\n[AWS Cloud Plugin](http://github.com/elasticsearch/elasticsearch-cloud-aws) will be installed on the node,\nallowing you to use the _Amazon_ AWS features: node auto-discovery and S3 gateway.\nYou may set your AWS credentials either in the “elasticsearch/aws” data bag,\nor directly in the node configuration.\n\nYou may want to include the `elasticsearch::proxy_nginx` recipe, which will configure _Nginx_ as\na reverse proxy for _elasticsearch_, so you may access it remotely with HTTP Authentication.\n(Be sure to include a `nginx` cookbook in your node setup in this case.)\n\nThe cookbook also provides a test case in the `files/default/tests/minitest/` directory,\nwhich can be executed as a part of the Chef run\n(via the [Minitest Chef Handler](https://github.com/calavera/minitest-chef-handler) support).\nIt checks the basic installation mechanics, populates the `test_chef_cookbook` index\nwith some sample data and performs a simple search.\n\n\nUsage\n-----\n\nInclude the `elasticsearch` recipe in the `run_list` of a node. Then, upload the cookbook to the _Chef_ server:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bash\n    knife cookbook upload elasticsearch\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nTo enable the _Amazon_ AWS related features, include the `elasticsearch::plugin_aws` recipe.\nYou will need to configure the AWS credentials, bucket names, etc.\n\nYou may do that in the node configuration (with `knife node edit MYNODE` or in the _Chef Server_ console),\nbut it is arguably more convenient to store the information in an \"elasticsearch\" _data bag_:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bash\n    mkdir -p ./data_bags/elasticsearch\n    echo '{ \n      \"id\" : \"aws\",\n      \"discovery\" : { \"type\": \"ec2\" },\n\n      \"gateway\" : {\n        \"type\"    : \"s3\",\n        \"s3\"      : { \"bucket\": \"YOUR BUCKET NAME\" }\n      },\n\n      \"cloud\"   : {\n        \"aws\"     : { \"access_key\": \"YOUR ACCESS KEY\", \"secret_key\": \"YOUR SECRET ACCESS KEY\" },\n        \"ec2\"     : { \"security_group\": \"elasticsearch\" }\n      }\n    }' >> ./data_bags/elasticsearch/aws.json\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nDo not forget to upload the data bag to the _Chef_ server:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bash\n    knife data bag from file elasticsearch aws.json\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nUsually, you will restrict the access to _elasticsearch_ with firewall rules. However, it's convenient\nto be able to connect to the _elasticsearch_ cluster from `curl` or a HTTP client, or to use a management tool such as\n[_BigDesk_](http://github.com/lukas-vlcek/bigdesk) or [_Paramedic_](http://github.com/karmi/elasticsearch-paramedic).\n\nTo enable authorized access to _elasticsearch_, you need to include the `elasticsearch::proxy_nginx` recipe,\nwhich will install, configure and run [_Nginx_](http://nginx.org) as a reverse proxy, allowing users with proper\ncredentials to connect.\n\nAs with AWS, you may store the usernames and passwords in the node configuration, but also in a data bag item:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bash\n    mkdir -p ./data_bags/elasticsearch\n    echo '{\n      \"id\" : \"users\",\n      \"users\" : [\n        {\"username\" : \"USERNAME\", \"password\" : \"PASSWORD\"},\n        {\"username\" : \"USERNAME\", \"password\" : \"PASSWORD\"}\n      ]\n    }\n    ' >> ./data_bags/elasticsearch/users.json\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAgain, do not forget to upload the data bag to the _Chef_ server:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bash\n    knife data bag from file elasticsearch users.json\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAfter you have configured the node and uploaded all the information to the _Chef_ server, run `chef-client` on the node(s):\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bash\n    knife ssh name:elasticsearch* 'sudo chef-client'\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nTesting with Vagrant\n--------------------\n\nThe cookbook comes with a [`Vagrantfile`](https://github.com/karmi/cookbook-elasticsearch/blob/master/Vagrantfile),\nallowing you to test-drive the installation and configuration with [_Vagrant_](http://vagrantup.com/),\na tool for building virtualized development infrastructures.\n\nFirst, make sure, you have both _VirtualBox_ and _Vagrant_\n[installed](http://vagrantup.com/docs/getting-started/index.html).\n\nThen, clone this repository into a `elasticsearch` directory on your development machine:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bash\n    git clone git://github.com/karmi/cookbook-elasticsearch.git elasticsearch\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSwitch to the cloned repository:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bash\n   cd elasticsearch\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nInstall the neccessary gems:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bash\n   bundle install\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou need to download the required third-party cookbooks (unless you already have them in `~/cookbooks`).\n\nThe easiest way is to use the bundled [_Berkshelf_](http://berkshelf.com) support:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bash\n  berks install --shims ./tmp/cookbooks\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nOf course, you can install the cookbooks manually as well:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bash\n    curl -# -L -k http://s3.amazonaws.com/community-files.opscode.com/cookbook_versions/tarballs/1184/original/apt.tgz   | tar xz -C tmp/cookbooks\n    curl -# -L -k http://s3.amazonaws.com/community-files.opscode.com/cookbook_versions/tarballs/1421/original/java.tgz  | tar xz -C tmp/cookbooks\n    curl -# -L -k http://s3.amazonaws.com/community-files.opscode.com/cookbook_versions/tarballs/1098/original/vim.tgz   | tar xz -C tmp/cookbooks\n    curl -# -L -k http://s3.amazonaws.com/community-files.opscode.com/cookbook_versions/tarballs/1413/original/nginx.tgz | tar xz -C tmp/cookbooks\n    curl -# -L -k http://s3.amazonaws.com/community-files.opscode.com/cookbook_versions/tarballs/915/original/monit.tgz  | tar xz -C tmp/cookbooks\n    curl -# -L -k http://s3.amazonaws.com/community-files.opscode.com/cookbook_versions/tarballs/1631/original/ark.tgz   | tar xz -C tmp/cookbooks\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe `Vagrantfile` supports three Linux distributions so far:\n\n* Ubuntu Lucid 32 bit\n* Ubuntu Lucid 64 bit\n* CentOS 6 32 bit\n\nUse the `vagrant status` command for more information.\n\nWe will use the [_Ubuntu Lucid 64_](http://vagrantup.com/v1/docs/boxes.html) box for the purpose of this demo.\nYou may want to test-drive this cookbook on a different distribution; check out the available boxes at <http://vagrantbox.es>.\n\nLaunch the virtual machine with _Vagrant_ (it will download the box unless you already have it):\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bash\n    time vagrant up lucid64\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe machine will be started and automatically provisioned with \n[_chef-solo_](http://vagrantup.com/v1/docs/provisioners/chef_solo.html).\n\nYou'll see _Chef_ debug messages flying by in your terminal, downloading, installing and configuring _Java_, _Nginx_,\n_elasticsearch_, and all the other components.\nThe process should take about 15 minutes on a reasonable machine and internet connection.\n\nAfter the process is done, you may connect to _elasticsearch_ via the _Nginx_ proxy from the outside:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bash\n    curl 'http://USERNAME:PASSWORD@33.33.33.10:8080/test_chef_cookbook/_search?pretty&q=*'\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nOf course, you should connect to the box with SSH and check things out:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bash\n    vagrant ssh lucid64\n\n    ps aux | grep elasticsearch\n    service elasticsearch status --verbose\n    curl http://localhost:9200/_cluster/health?pretty\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nCookbook Organization\n---------------------\n\n* `attributes/default.rb`: version, paths, memory and naming settings for the node\n* `attributes/plugin_aws.rb`: AWS settings\n* `attributes/proxy_nginx.rb`: _Nginx_ settings\n* `templates/default/elasticsearch.init.erb`: service init script\n* `templates/default/elasticsearch.yml.erb`: main _elasticsearch_ configuration file\n* `templates/default/elasticsearch-env.sh.erb`: environment variables needed by the _Java Virtual Machine_ and _elasticsearch_\n* `templates/default/elasticsearch_proxy_nginx.conf.erb`: the reverse proxy configuration\n* `templates/default/elasticsearch.conf.erb`: _Monit_ configuration file\n* `files/default/tests/minitest`: integration tests\n\nLicense\n-------\n\nAuthor: Karel Minarik (<karmi@karmi.cz>)\n\nMIT LICENSE\n",
  "maintainer": "karmi",
  "maintainer_email": "karmi@karmi.cz",
  "license": "MIT License",
  "platforms": {
  },
  "dependencies": {
    "ark": ">= 0.0.0"
  },
  "recommendations": {
    "java": ">= 0.0.0",
    "monit": ">= 0.0.0",
    "nginx": ">= 0.0.0"
  },
  "suggestions": {
  },
  "conflicting": {
  },
  "providing": {
    "elasticsearch": ">= 0.0.0",
    "service[elasticsearch]": ">= 0.0.0",
    "install_plugin(:plugin_name)": ">= 0.0.0"
  },
  "replacing": {
  },
  "attributes": {
  },
  "groupings": {
  },
  "recipes": {
  },
  "version": "0.0.2"
}